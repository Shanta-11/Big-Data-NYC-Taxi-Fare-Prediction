{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 17:32:03 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.60 instead (on interface enp3s0)\n",
      "23/12/02 17:32:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/02 17:32:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import year, month, hour, sum as spark_sum, col, dayofweek\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "spark=SparkSession.builder.appName(\"Testing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_dir = 'db'\n",
    "\n",
    "# Get a list of all Parquet files in the directory\n",
    "parquet_files = [os.path.join(parquet_dir, file) for file in os.listdir(parquet_dir) if file.endswith('.parquet')]\n",
    "\n",
    "# Read the Parquet files into separate DataFrames\n",
    "dataframes = [spark.read.parquet(file) for file in parquet_files]\n",
    "\n",
    "# Combine DataFrames into a single DataFrame\n",
    "# combined_df = dataframes[0].union(*dataframes[1:])\n",
    "\n",
    "origDF = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    origDF = origDF.union(df)\n",
    "\n",
    "# origDF = spark.read.parquet('yellow_tripdata_2023-06.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "origDF = origDF.drop(\"VendorID\", \"passenger_count\", \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\", \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \"congestion_surcharge\", \"Airport_fee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDF = origDF.withColumn(\"year\", year(\"tpep_pickup_datetime\")) \\\n",
    "                             .withColumn(\"month\", month(\"tpep_pickup_datetime\")) \\\n",
    "                             .withColumn(\"hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "                             .withColumn(\"day_of_week\", dayofweek(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_duration = origDF.withColumn(\n",
    "    \"duration\",\n",
    "    (origDF.tpep_dropoff_datetime - origDF.tpep_pickup_datetime).cast(\"int\") # Duration in minutes\n",
    ")\n",
    "origDF = df_with_duration.drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDF = origDF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDF = origDF.filter(origDF['duration']<20000)\n",
    "origDF = origDF.filter(origDF['duration']>180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDF = origDF.filter(origDF['trip_distance'] < 50)\n",
    "origDF = origDF.filter(origDF['trip_distance'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDF = origDF.filter(origDF['total_amount'] < 500)\n",
    "origDF = origDF.filter(origDF['total_amount'] >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------------+------------+----+-----+----+-----------+--------+\n",
      "|trip_distance|PULocationID|DOLocationID|total_amount|year|month|hour|day_of_week|duration|\n",
      "+-------------+------------+------------+------------+----+-----+----+-----------+--------+\n",
      "|          3.4|         140|         238|        33.6|2023|    6|   0|          5|    1253|\n",
      "|          3.4|          50|         151|        23.6|2023|    6|   0|          5|     614|\n",
      "|         10.2|         138|          97|       60.05|2023|    6|   0|          5|    1123|\n",
      "|         9.83|         100|         244|       53.28|2023|    6|   0|          5|    1406|\n",
      "|         1.17|         137|         234|       15.02|2023|    6|   0|          5|     514|\n",
      "|          3.6|         249|          33|       28.05|2023|    6|   0|          5|     796|\n",
      "|         3.08|         141|         226|        26.8|2023|    6|   0|          5|    1136|\n",
      "|          1.1|         246|          50|        18.0|2023|    6|   0|          5|     527|\n",
      "|         0.99|         186|         234|       13.22|2023|    6|   0|          5|     237|\n",
      "|         5.43|         234|         166|       33.96|2023|    6|   0|          5|     988|\n",
      "|         1.68|         249|         170|       18.84|2023|    6|   0|          5|     479|\n",
      "|          2.1|          50|         158|        22.2|2023|    6|   0|          5|     736|\n",
      "|         3.72|         239|          90|        30.6|2023|    6|   0|          5|    1100|\n",
      "|         0.98|         249|         125|       14.64|2023|    6|   0|          5|     290|\n",
      "|         1.91|         148|         170|        17.4|2023|    6|   0|          5|     562|\n",
      "|         1.42|         137|         229|        16.0|2023|    6|   0|          5|     320|\n",
      "|        25.36|         132|         228|      117.96|2023|    6|   0|          5|    2233|\n",
      "|        22.98|         132|         241|      117.36|2023|    6|   0|          5|    2361|\n",
      "|         0.75|         186|         137|       16.32|2023|    6|   0|          5|     521|\n",
      "|          1.9|         234|         230|        21.2|2023|    6|   0|          5|    1332|\n",
      "+-------------+------------+------------+------------+----+-----+----+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "origDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Indexing and Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"trip_distance\", \"PULocationID\", \"DOLocationID\", \"year\", \"month\", \"hour\", \"day_of_week\", \"duration\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df = assembler.transform(origDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=265).fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|total_amount|            features|     indexedFeatures|\n",
      "+------------+--------------------+--------------------+\n",
      "|        33.6|[3.4,140.0,238.0,...|[3.4,136.0,234.0,...|\n",
      "|        23.6|[3.4,50.0,151.0,2...|[3.4,49.0,148.0,3...|\n",
      "|       60.05|[10.2,138.0,97.0,...|[10.2,134.0,96.0,...|\n",
      "|       53.28|[9.83,100.0,244.0...|[9.83,99.0,240.0,...|\n",
      "|       15.02|[1.17,137.0,234.0...|[1.17,133.0,230.0...|\n",
      "|       28.05|[3.6,249.0,33.0,2...|[3.6,243.0,32.0,3...|\n",
      "|        26.8|[3.08,141.0,226.0...|[3.08,137.0,222.0...|\n",
      "|        18.0|[1.1,246.0,50.0,2...|[1.1,240.0,49.0,3...|\n",
      "|       13.22|[0.99,186.0,234.0...|[0.99,181.0,230.0...|\n",
      "|       33.96|[5.43,234.0,166.0...|[5.43,228.0,163.0...|\n",
      "|       18.84|[1.68,249.0,170.0...|[1.68,243.0,167.0...|\n",
      "|        22.2|[2.1,50.0,158.0,2...|[2.1,49.0,155.0,3...|\n",
      "|        30.6|[3.72,239.0,90.0,...|[3.72,233.0,89.0,...|\n",
      "|       14.64|[0.98,249.0,125.0...|[0.98,243.0,122.0...|\n",
      "|        17.4|[1.91,148.0,170.0...|[1.91,144.0,167.0...|\n",
      "|        16.0|[1.42,137.0,229.0...|[1.42,133.0,225.0...|\n",
      "|      117.96|[25.36,132.0,228....|[25.36,128.0,224....|\n",
      "|      117.36|[22.98,132.0,241....|[22.98,128.0,237....|\n",
      "|       16.32|[0.75,186.0,137.0...|[0.75,181.0,134.0...|\n",
      "|        21.2|[1.9,234.0,230.0,...|[1.9,228.0,226.0,...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = featureIndexer.transform(df)\n",
    "fin_df = df.drop(\"trip_distance\",\"PULocationID\", \"DOLocationID\", \"year\", \"month\", \"hour\", \"day_of_week\", \"duration\")\n",
    "fin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = fin_df.randomSplit([0.7, 0.3], seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 17:33:29 WARN MemoryStore: Not enough space to cache rdd_73_26 in memory! (computed 44.4 MiB so far)\n",
      "23/12/02 17:33:29 WARN BlockManager: Persisting block rdd_73_26 to disk instead.\n",
      "23/12/02 17:33:29 WARN MemoryStore: Not enough space to cache rdd_73_30 in memory! (computed 44.4 MiB so far)\n",
      "23/12/02 17:33:29 WARN BlockManager: Persisting block rdd_73_30 to disk instead.\n",
      "23/12/02 17:33:32 WARN MemoryStore: Not enough space to cache rdd_73_26 in memory! (computed 44.4 MiB so far)\n",
      "23/12/02 17:33:32 WARN MemoryStore: Not enough space to cache rdd_73_30 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:33:38 WARN MemoryStore: Not enough space to cache rdd_73_17 in memory! (computed 44.4 MiB so far)\n",
      "23/12/02 17:33:38 WARN BlockManager: Persisting block rdd_73_17 to disk instead.\n",
      "23/12/02 17:33:41 WARN MemoryStore: Not enough space to cache rdd_73_5 in memory! (computed 149.9 MiB so far)\n",
      "23/12/02 17:33:41 WARN BlockManager: Persisting block rdd_73_5 to disk instead.\n",
      "23/12/02 17:33:45 WARN MemoryStore: Not enough space to cache rdd_73_17 in memory! (computed 236.8 MiB so far)\n",
      "23/12/02 17:33:48 WARN MemoryStore: Not enough space to cache rdd_73_5 in memory! (computed 236.8 MiB so far)\n",
      "23/12/02 17:33:51 WARN MemoryStore: Not enough space to cache rdd_73_30 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:33:51 WARN MemoryStore: Not enough space to cache rdd_73_5 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:33:51 WARN MemoryStore: Not enough space to cache rdd_73_26 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:33:51 WARN MemoryStore: Not enough space to cache rdd_73_17 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:33:57 WARN MemoryStore: Not enough space to cache rdd_73_5 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:33:57 WARN MemoryStore: Not enough space to cache rdd_73_30 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:33:57 WARN MemoryStore: Not enough space to cache rdd_73_17 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:33:57 WARN MemoryStore: Not enough space to cache rdd_73_26 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:34:06 WARN MemoryStore: Not enough space to cache rdd_73_17 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:34:06 WARN MemoryStore: Not enough space to cache rdd_73_30 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:34:06 WARN MemoryStore: Not enough space to cache rdd_73_5 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:34:06 WARN MemoryStore: Not enough space to cache rdd_73_26 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:34:16 WARN MemoryStore: Not enough space to cache rdd_73_17 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:34:16 WARN MemoryStore: Not enough space to cache rdd_73_30 in memory! (computed 66.6 MiB so far)\n",
      "23/12/02 17:34:16 WARN MemoryStore: Not enough space to cache rdd_73_5 in memory! (computed 99.9 MiB so far)\n",
      "23/12/02 17:34:16 WARN MemoryStore: Not enough space to cache rdd_73_26 in memory! (computed 99.9 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gbt = RandomForestRegressor(featuresCol='indexedFeatures', labelCol='total_amount', seed=42, maxBins=300)\n",
    "model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/shantanu/.local/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.4.1.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.913072400062082"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_eval = RegressionEvaluator(labelCol='total_amount', metricName='rmse')\n",
    "rmse = rmse_eval.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
